{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook I'll reconstruct the process and results of the DTCR algorithm presented in the \"Learning Representations for Time Series Clustering\"."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Imports\r\n",
    "import torch\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from Utilities.DTCR import DTCRModel, DTCRConfig\r\n",
    "from Utilities.UCRParser import read_dataset\r\n",
    "from Utilities.DRNN import BidirectionalDRNN"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "train_ds, test_ds = read_dataset(\"Two_Patterns\")\r\n",
    "train_dl = DataLoader(train_ds, batch_size=2, shuffle=True)\r\n",
    "test_dl = DataLoader(test_ds, batch_size=2, shuffle=True)\r\n",
    "\r\n",
    "config = DTCRConfig()\r\n",
    "config.class_num = train_ds.number_of_labels\r\n",
    "config.input_size = train_ds[0][0].shape[1]\r\n",
    "config.num_steps = train_ds[0][0].shape[0]\r\n",
    "config.batch_size = 2\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading the Two_Patterns dataset...\n",
      "The dataset Two_Patterns was loaded.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "dtcr_model = DTCRModel(config)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "encoder = dtcr_model.encoder\r\n",
    "encoder\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BidirectionalDRNN(\n",
       "  (_regular_drnn): DRNN(\n",
       "    (cells): Sequential(\n",
       "      (0): GRU(1, 100)\n",
       "      (1): GRU(100, 50)\n",
       "      (2): GRU(50, 50)\n",
       "    )\n",
       "  )\n",
       "  (_backwards_drnn): DRNN(\n",
       "    (cells): Sequential(\n",
       "      (0): GRU(1, 100)\n",
       "      (1): GRU(100, 50)\n",
       "      (2): GRU(50, 50)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "decoder = dtcr_model.decoder\r\n",
    "decoder"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DTCRDecoder(\n",
       "  (_rnn): GRU(400, 400, batch_first=True)\n",
       "  (_linear): Linear(in_features=400, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "loss = torch.nn.MSELoss()\r\n",
    "opt = torch.optim.Adam(list(decoder.parameters()) + list(encoder.parameters()))#, eps=5e-3)\r\n",
    "print_interval = 5\r\n",
    "for epoch in range(2):\r\n",
    "    running_loss = 0.0\r\n",
    "    for index, (sample_data, sample_label) in enumerate(train_dl):\r\n",
    "        opt.zero_grad()\r\n",
    "\r\n",
    "        _, hidden_outputs = encoder(sample_data)\r\n",
    "        latent_representation = dtcr_model.get_latent_representation(hidden_outputs)\r\n",
    "\r\n",
    "        #repr_for_reconstruction = dtcr_model.prepare_representation_for_reconstruction(\r\n",
    "        #    latent_representation)\r\n",
    "        #reconstructed_inputs, _ = decoder(repr_for_reconstruction)\r\n",
    "        reconstructed_inputs, _ = decoder(latent_representation)\r\n",
    "        # reconstructed_inputs: (N, seq_len, hidden_size), we need just the last of each\r\n",
    "        # hidden_size\r\n",
    "        output = loss(reconstructed_inputs, sample_data)\r\n",
    "        output.backward()\r\n",
    "        opt.step()\r\n",
    "\r\n",
    "        running_loss += output.item()\r\n",
    "        if index % print_interval == print_interval - 1:  # print every 2000 mini-batches\r\n",
    "            print('[%d, %5d] loss: %.3f' %\r\n",
    "                (epoch + 1, index + 1, running_loss / print_interval))\r\n",
    "            running_loss = 0.0\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\Anaconda3\\envs\\NLP\\lib\\site-packages\\torch\\nn\\modules\\loss.py:445: UserWarning: Using a target size (torch.Size([2, 128, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1,     5] loss: 1.057\n",
      "[1,    10] loss: 1.010\n",
      "[1,    15] loss: 1.001\n",
      "[1,    20] loss: 0.994\n",
      "[1,    25] loss: 0.995\n",
      "[1,    30] loss: 0.993\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12248/322724001.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# hidden_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreconstructed_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\NLP\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\NLP\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "reconstructed_inputs[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.0003],\n",
       "        [-0.0006],\n",
       "        [-0.0009],\n",
       "        [-0.0012],\n",
       "        [-0.0015],\n",
       "        [-0.0018],\n",
       "        [-0.0021],\n",
       "        [-0.0024],\n",
       "        [-0.0027],\n",
       "        [-0.0030],\n",
       "        [-0.0033],\n",
       "        [-0.0036],\n",
       "        [-0.0039],\n",
       "        [-0.0042],\n",
       "        [-0.0045],\n",
       "        [-0.0048],\n",
       "        [-0.0051],\n",
       "        [-0.0053],\n",
       "        [-0.0056],\n",
       "        [-0.0059],\n",
       "        [-0.0062],\n",
       "        [-0.0065],\n",
       "        [-0.0068],\n",
       "        [-0.0071],\n",
       "        [-0.0074],\n",
       "        [-0.0077],\n",
       "        [-0.0080],\n",
       "        [-0.0082],\n",
       "        [-0.0085],\n",
       "        [-0.0088],\n",
       "        [-0.0091],\n",
       "        [-0.0094],\n",
       "        [-0.0097],\n",
       "        [-0.0100],\n",
       "        [-0.0102],\n",
       "        [-0.0105],\n",
       "        [-0.0108],\n",
       "        [-0.0111],\n",
       "        [-0.0114],\n",
       "        [-0.0117],\n",
       "        [-0.0119],\n",
       "        [-0.0122],\n",
       "        [-0.0125],\n",
       "        [-0.0128],\n",
       "        [-0.0131],\n",
       "        [-0.0133],\n",
       "        [-0.0136],\n",
       "        [-0.0139],\n",
       "        [-0.0142],\n",
       "        [-0.0145],\n",
       "        [-0.0147],\n",
       "        [-0.0150],\n",
       "        [-0.0153],\n",
       "        [-0.0156],\n",
       "        [-0.0158],\n",
       "        [-0.0161],\n",
       "        [-0.0164],\n",
       "        [-0.0167],\n",
       "        [-0.0169],\n",
       "        [-0.0172],\n",
       "        [-0.0175],\n",
       "        [-0.0177],\n",
       "        [-0.0180],\n",
       "        [-0.0183],\n",
       "        [-0.0186],\n",
       "        [-0.0188],\n",
       "        [-0.0191],\n",
       "        [-0.0194],\n",
       "        [-0.0196],\n",
       "        [-0.0199],\n",
       "        [-0.0202],\n",
       "        [-0.0204],\n",
       "        [-0.0207],\n",
       "        [-0.0210],\n",
       "        [-0.0212],\n",
       "        [-0.0215],\n",
       "        [-0.0218],\n",
       "        [-0.0220],\n",
       "        [-0.0223],\n",
       "        [-0.0225],\n",
       "        [-0.0228],\n",
       "        [-0.0231],\n",
       "        [-0.0233],\n",
       "        [-0.0236],\n",
       "        [-0.0239],\n",
       "        [-0.0241],\n",
       "        [-0.0244],\n",
       "        [-0.0246],\n",
       "        [-0.0249],\n",
       "        [-0.0252],\n",
       "        [-0.0254],\n",
       "        [-0.0257],\n",
       "        [-0.0259],\n",
       "        [-0.0262],\n",
       "        [-0.0264],\n",
       "        [-0.0267],\n",
       "        [-0.0270],\n",
       "        [-0.0272],\n",
       "        [-0.0275],\n",
       "        [-0.0277],\n",
       "        [-0.0280],\n",
       "        [-0.0282],\n",
       "        [-0.0285],\n",
       "        [-0.0287],\n",
       "        [-0.0290],\n",
       "        [-0.0292],\n",
       "        [-0.0295],\n",
       "        [-0.0297],\n",
       "        [-0.0300],\n",
       "        [-0.0302],\n",
       "        [-0.0305],\n",
       "        [-0.0307],\n",
       "        [-0.0310],\n",
       "        [-0.0312],\n",
       "        [-0.0315],\n",
       "        [-0.0317],\n",
       "        [-0.0320],\n",
       "        [-0.0322],\n",
       "        [-0.0325],\n",
       "        [-0.0327],\n",
       "        [-0.0330],\n",
       "        [-0.0332],\n",
       "        [-0.0335],\n",
       "        [-0.0337],\n",
       "        [-0.0340],\n",
       "        [-0.0342],\n",
       "        [-0.0344],\n",
       "        [-0.0347]], grad_fn=<SelectBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sample_data[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.1375],\n",
       "        [-0.1239],\n",
       "        [ 0.4653],\n",
       "        [ 0.0880],\n",
       "        [ 0.2117],\n",
       "        [ 0.5962],\n",
       "        [ 0.0431],\n",
       "        [ 0.1223],\n",
       "        [ 0.3642],\n",
       "        [-0.5954],\n",
       "        [ 0.0387],\n",
       "        [ 0.2478],\n",
       "        [-0.2279],\n",
       "        [ 0.3670],\n",
       "        [ 0.1559],\n",
       "        [ 0.1879],\n",
       "        [ 0.6380],\n",
       "        [-0.0640],\n",
       "        [-0.3787],\n",
       "        [-0.2925],\n",
       "        [ 0.1132],\n",
       "        [ 0.1191],\n",
       "        [ 0.2278],\n",
       "        [ 0.3051],\n",
       "        [ 0.3733],\n",
       "        [-0.2286],\n",
       "        [ 0.0054],\n",
       "        [ 0.0323],\n",
       "        [-0.1088],\n",
       "        [-0.0878],\n",
       "        [-0.4618],\n",
       "        [-0.7201],\n",
       "        [ 0.1494],\n",
       "        [ 0.3238],\n",
       "        [ 0.1462],\n",
       "        [-0.0580],\n",
       "        [ 0.4726],\n",
       "        [-0.1032],\n",
       "        [-0.3556],\n",
       "        [ 0.0334],\n",
       "        [-0.6799],\n",
       "        [ 0.3069],\n",
       "        [ 0.3108],\n",
       "        [ 0.1048],\n",
       "        [-0.2105],\n",
       "        [ 0.4394],\n",
       "        [-0.6251],\n",
       "        [ 0.2421],\n",
       "        [ 0.3760],\n",
       "        [-0.7003],\n",
       "        [-0.1711],\n",
       "        [ 0.0826],\n",
       "        [-0.1600],\n",
       "        [-0.0640],\n",
       "        [ 0.0580],\n",
       "        [-0.4089],\n",
       "        [ 0.4755],\n",
       "        [ 0.0304],\n",
       "        [ 0.4585],\n",
       "        [-0.6206],\n",
       "        [-0.0827],\n",
       "        [ 1.7098],\n",
       "        [ 1.7098],\n",
       "        [ 1.7098],\n",
       "        [ 1.7098],\n",
       "        [ 1.7098],\n",
       "        [ 1.7098],\n",
       "        [ 1.7098],\n",
       "        [ 1.7098],\n",
       "        [ 1.7098],\n",
       "        [ 1.7098],\n",
       "        [ 1.7098],\n",
       "        [ 1.7098],\n",
       "        [-1.7227],\n",
       "        [-1.7227],\n",
       "        [-1.7227],\n",
       "        [-1.7227],\n",
       "        [-1.7227],\n",
       "        [-1.7227],\n",
       "        [-1.7227],\n",
       "        [-1.7227],\n",
       "        [-1.7227],\n",
       "        [-1.7227],\n",
       "        [-1.7227],\n",
       "        [-1.7227],\n",
       "        [-0.0556],\n",
       "        [ 0.1695],\n",
       "        [-0.3455],\n",
       "        [-0.5313],\n",
       "        [ 0.3392],\n",
       "        [ 0.1306],\n",
       "        [-0.5350],\n",
       "        [ 0.2089],\n",
       "        [-0.4350],\n",
       "        [ 0.3805],\n",
       "        [-0.0749],\n",
       "        [ 0.0297],\n",
       "        [ 0.2150],\n",
       "        [-0.1934],\n",
       "        [-0.4286],\n",
       "        [ 0.0096],\n",
       "        [ 1.7098],\n",
       "        [ 1.7098],\n",
       "        [ 1.7098],\n",
       "        [ 1.7098],\n",
       "        [ 1.7098],\n",
       "        [ 1.7098],\n",
       "        [ 1.7098],\n",
       "        [ 1.7098],\n",
       "        [-1.7227],\n",
       "        [-1.7227],\n",
       "        [-1.7227],\n",
       "        [-1.7227],\n",
       "        [-1.7227],\n",
       "        [-1.7227],\n",
       "        [-1.7227],\n",
       "        [-1.7227],\n",
       "        [-0.2549],\n",
       "        [ 0.2783],\n",
       "        [-0.0825],\n",
       "        [-0.0207],\n",
       "        [-0.3773],\n",
       "        [-0.1353],\n",
       "        [ 0.7042],\n",
       "        [ 0.0711],\n",
       "        [-0.1314],\n",
       "        [ 0.0433],\n",
       "        [ 0.2329]])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edcda8d2a73236de09051cdde745a65eb7ed2a749b88f01ef4a8b567e29cd136"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('NLP': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}