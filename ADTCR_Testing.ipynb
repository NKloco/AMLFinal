{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from Utilities.DTCR import DTCRConfig\n",
    "from Utilities.ADTCR import ADTCRModel\n",
    "from Utilities.UCRParser import read_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Symbols dataset...\n",
      "The dataset Symbols was loaded.\n"
     ]
    }
   ],
   "source": [
    "testing_dataset = \"Symbols\"\n",
    "train_ds, test_ds = read_dataset(testing_dataset)\n",
    "\n",
    "config = DTCRConfig()\n",
    "config.checkpoint_path = \"ADTCR_Checkpoints\"\n",
    "config.class_num = train_ds.number_of_labels\n",
    "config.input_size = train_ds[0][0].shape[1]\n",
    "config.num_steps = train_ds[0][0].shape[0]\n",
    "config.model_name = testing_dataset\n",
    "config.batch_size = len(train_ds)\n",
    "\n",
    "config.learning_rate = 8e-5\n",
    "config.checkpoint_interval = 50\n",
    "config.hidden_size = [50, 30, 30]\n",
    "config.dilations = [1, 4, 16]\n",
    "config.coefficient_lambda = 0.1\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True, drop_last=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=len(test_ds), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADTCRModel(\n",
       "  (encoder): AttentionEncoder(\n",
       "    (input_embedding): Linear(in_features=1, out_features=220, bias=True)\n",
       "    (positional_embedding): Embedding(398, 220)\n",
       "    (layers): ModuleList(\n",
       "      (0): AttentionEncoderLayer(\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=220, out_features=220, bias=True)\n",
       "        )\n",
       "        (first_norm): LayerNorm((220,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): Sequential(\n",
       "          (0): Linear(in_features=220, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=220, bias=True)\n",
       "        )\n",
       "        (second_norm): LayerNorm((220,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): AttentionEncoderLayer(\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=220, out_features=220, bias=True)\n",
       "        )\n",
       "        (first_norm): LayerNorm((220,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): Sequential(\n",
       "          (0): Linear(in_features=220, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=220, bias=True)\n",
       "        )\n",
       "        (second_norm): LayerNorm((220,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): AttentionEncoderLayer(\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=220, out_features=220, bias=True)\n",
       "        )\n",
       "        (first_norm): LayerNorm((220,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): Sequential(\n",
       "          (0): Linear(in_features=220, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=220, bias=True)\n",
       "        )\n",
       "        (second_norm): LayerNorm((220,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): AttentionEncoderLayer(\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=220, out_features=220, bias=True)\n",
       "        )\n",
       "        (first_norm): LayerNorm((220,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): Sequential(\n",
       "          (0): Linear(in_features=220, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=220, bias=True)\n",
       "        )\n",
       "        (second_norm): LayerNorm((220,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): AttentionEncoderLayer(\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=220, out_features=220, bias=True)\n",
       "        )\n",
       "        (first_norm): LayerNorm((220,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): Sequential(\n",
       "          (0): Linear(in_features=220, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=220, bias=True)\n",
       "        )\n",
       "        (second_norm): LayerNorm((220,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): AttentionEncoderLayer(\n",
       "        (mha): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=220, out_features=220, bias=True)\n",
       "        )\n",
       "        (first_norm): LayerNorm((220,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): Sequential(\n",
       "          (0): Linear(in_features=220, out_features=1024, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1024, out_features=220, bias=True)\n",
       "        )\n",
       "        (second_norm): LayerNorm((220,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): DTCRDecoder(\n",
       "    (_rnn): GRU(220, 220, batch_first=True)\n",
       "    (_linear): Linear(in_features=220, out_features=1, bias=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=220, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
       "    (3): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtcr_model = ADTCRModel(config)\n",
    "dtcr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to continue the training load the model here from the checkpoint\n",
    "# dtcr_model = torch.load(\"Checkpoints/ECGFiveDays_900\")\n",
    "recons_criterion = config.decoding_criterion()\n",
    "classify_criterion = config.classifier_criterion()\n",
    "optimizer = config.optimizer(dtcr_model.parameters(),\n",
    "                             eps=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_73248/408261322.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\nitz_\\AppData\\Local\\Temp/ipykernel_73248/408261322.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    epochs = 1500ppppppppppppppp\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "epochs = 1500\n",
    "for epoch in range(dtcr_model._training_iteration, epochs):\n",
    "    print(\"Starting epoch {}:\".format(epoch + 1))\n",
    "    dtcr_model.train_step(train_dl, test_dl, recons_criterion, classify_criterion,\n",
    "                          optimizer)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "be203ce0b3afc4f5c37fbac412025d7ed1d67cabe9dd00b1fc8774c6d6d19d70"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
